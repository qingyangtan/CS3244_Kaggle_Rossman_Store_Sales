{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crphang/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "## importing the models that will be trialed and tested against validation dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.read_csv('store.csv')\n",
    "train = pd.read_csv('train.csv',dtype={\"StateHoliday\": str})\n",
    "test = pd.read_csv('test.csv',dtype={\"StateHoliday\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions that help initialise data and include new features to the data set to be trained\n",
    "\"\"\"\n",
    "def initialise_train_data(train, store):\n",
    "    ## label our dataframe with id\n",
    "    train['Id'] = range(1,len(train)+1)\n",
    "    ## fill the N.A.N values\n",
    "    store = store.fillna(0)\n",
    "    ## combine all features together\n",
    "    df = train.merge(store, on='Store')\n",
    "    ## Get labels and remove from dataframe\n",
    "    labels = df.values[:,3]\n",
    "    labels = np.array([labels], dtype=np.int32).T\n",
    "\n",
    "    return (df, labels)\n",
    "\n",
    "def initialise_test_data(test, store):\n",
    "    ## label our dataframe with id\n",
    "    test['Id'] = range(1,len(test)+1)\n",
    "    ## fill the N.A.N values\n",
    "    store = store.fillna(0)\n",
    "    ## combine all features together\n",
    "    df = test.merge(store, on='Store')\n",
    "    df = df.sort_values(['Id'], ascending=[1])\n",
    "    return df\n",
    "\n",
    "def set_store_close_and_sales_data(df):\n",
    "    df['StoreClosedNextDay'] = df['Open']\n",
    "    df['StoreClosedNextDay'] = df.StoreClosedNextDay.shift(-1)\n",
    "    df['StoreClosedNextDay'] = -1 * (df['StoreClosedNextDay'] - 1)\n",
    "    df['StoreClosedNextDay'][df.shape[0] - 1] = 0\n",
    "    df['StoreClosedNextDay'] = df.StoreClosedNextDay.astype(int)\n",
    "\n",
    "    if 'Sales' in df.columns:\n",
    "        df = df[df.Sales != 0]\n",
    "        df.drop('Sales', axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "## converting dates into year, month, day and additional feature week of the year\n",
    "def date_convert(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['WeekOfYear'] = df['Date'].dt.weekofyear\n",
    "    return df\n",
    "\n",
    "## adjust and standardise the mappings for all the categorical variables.\n",
    "def mapping_encoding(df):\n",
    "    mappings = {'0': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4}\n",
    "    \n",
    "    ## replace with values so they can be one hot encoded\n",
    "    df.StoreType.replace(mappings, inplace=True)\n",
    "    df.Assortment.replace(mappings, inplace=True)\n",
    "    df.StateHoliday.replace(mappings, inplace=True)\n",
    "    df.PromoInterval.replace(mappings, inplace=True)\n",
    "    \n",
    "    df['StateHoliday'] = LabelEncoder().fit_transform(df['StateHoliday'])\n",
    "    df['Assortment'] = LabelEncoder().fit_transform(df['Assortment'])\n",
    "    df['StoreType'] = LabelEncoder().fit_transform(df['StoreType'])\n",
    "    return df\n",
    "\n",
    "def store_features(df):\n",
    "    mappings_month = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n",
    "                      7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    df['monthStr'] = df.Month.map(mappings_month)\n",
    "    df.loc[df.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    df['IsPromoMonth'] = 0\n",
    "    for interval in df.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                df.loc[(df.monthStr == month) & (df.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "    return df\n",
    "\n",
    "def select_features(df, feature_type):\n",
    "    train_features = ['Store', 'Promo', 'Promo2', 'Customers', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'IsPromoMonth', 'StoreClosedNextDay']\n",
    "    test_features = ['Id', 'Store', 'Promo', 'Promo2', 'Customers', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'IsPromoMonth', 'StoreClosedNextDay']\n",
    "\n",
    "    if feature_type == 'train':\n",
    "        return df[train_features]\n",
    "    return df[test_features]\n",
    "\n",
    "## check for rows which stores are closed\n",
    "def get_closed_stores_index(df):\n",
    "    return df.ix[df['Open']==0].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crphang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:86: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/home/crphang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build Features in the order defined by feature_builders array\n",
    "\"\"\"\n",
    "closed_index = get_closed_stores_index(test)\n",
    "\n",
    "train, labels = initialise_train_data(train, store)\n",
    "test = initialise_test_data(test, store)\n",
    "\n",
    "feature_builders = [set_store_close_and_sales_data, date_convert, mapping_encoding, store_features]\n",
    "\n",
    "for i in range(len(feature_builders)):\n",
    "    train = feature_builders[i](train)\n",
    "    test = feature_builders[i](test)\n",
    "\n",
    "train = select_features(train, 'train')\n",
    "test = select_features(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Store  Promo  Promo2  Customers  SchoolHoliday  StoreType  \\\n",
      "0          1      1      1       0        555              1          2   \n",
      "30         2      2      1       1        625              1          0   \n",
      "60         3      3      1       1        821              1          0   \n",
      "90         4      4      1       0       1498              1          2   \n",
      "120        5      5      1       0        559              1          0   \n",
      "150        6      6      1       0        589              1          0   \n",
      "180        7      7      1       0       1414              1          0   \n",
      "210        8      8      1       0        833              1          0   \n",
      "240        9      9      1       0        687              1          0   \n",
      "270       10     10      1       0        681              1          0   \n",
      "300       11     11      1       1       1236              1          0   \n",
      "330       12     12      1       1        962              1          0   \n",
      "360       13     13      1       1        568              0          3   \n",
      "390       14     14      1       1        710              1          0   \n",
      "420       15     15      1       1        766              1          3   \n",
      "450       16     16      1       0        979              1          0   \n",
      "480       17     17      1       1        946              1          0   \n",
      "510       18     18      1       1        936              1          3   \n",
      "540       19     19      1       1        718              1          0   \n",
      "570       20     20      1       1        974              0          3   \n",
      "600       21     21      1       1        682              1          2   \n",
      "630       22     22      1       1        633              0          0   \n",
      "660       23     23      1       0        560              1          3   \n",
      "690       24     24      1       1       1082              1          0   \n",
      "720       25     25      1       0       1586              1          2   \n",
      "750       26     26      1       0        611              1          3   \n",
      "780       27     27      1       1       1263              1          0   \n",
      "810       28     28      1       1        663              1          0   \n",
      "840       29     29      1       0        737              1          3   \n",
      "870       30     30      1       1        625              1          0   \n",
      "...      ...    ...    ...     ...        ...            ...        ...   \n",
      "32579  33421   1086      1       1        786              0          0   \n",
      "32609  33422   1087      1       0        661              0          3   \n",
      "32639  33423   1088      1       1        474              1          0   \n",
      "32669  33424   1089      1       0        973              1          3   \n",
      "32699  33425   1090      1       1        834              0          0   \n",
      "32729  33426   1091      1       0        738              0          0   \n",
      "32759  33427   1092      1       1       1007              0          0   \n",
      "32789  33428   1093      1       0        903              0          2   \n",
      "32819  33429   1094      1       1        350              0          3   \n",
      "32849  33430   1095      1       1        656              0          0   \n",
      "32879  33431   1096      1       1        631              1          0   \n",
      "32909  33432   1097      1       0       3070              0          1   \n",
      "32939  33433   1098      1       0        690              0          0   \n",
      "32969  33434   1099      1       1       1208              0          0   \n",
      "32999  33435   1100      1       1        567              1          0   \n",
      "33029  33436   1101      1       0       1011              0          3   \n",
      "33059  33437   1102      1       1        676              0          0   \n",
      "33089  33438   1103      1       1        410              0          3   \n",
      "33119  33439   1104      1       1        420              0          3   \n",
      "33149  33440   1105      1       1        511              1          2   \n",
      "33179  33441   1106      1       1        580              0          0   \n",
      "33209  33442   1107      1       1        625              0          0   \n",
      "33239  33443   1108      1       0        667              0          0   \n",
      "33269  33444   1109      1       1        424              0          2   \n",
      "33299  33445   1110      1       0        626              0          2   \n",
      "33329  33446   1111      1       1        329              1          0   \n",
      "33359  33447   1112      1       0        577              1          2   \n",
      "33389  33448   1113      1       0        793              0          0   \n",
      "33419  33449   1114      1       0       3784              0          0   \n",
      "33449  33450   1115      1       1        452              0          3   \n",
      "\n",
      "       Assortment  StateHoliday  DayOfWeek  Month  Day  Year  WeekOfYear  \\\n",
      "0               0             0          5      7   31  2015          31   \n",
      "30              0             0          5      7   31  2015          31   \n",
      "60              0             0          5      7   31  2015          31   \n",
      "90              2             0          5      7   31  2015          31   \n",
      "120             0             0          5      7   31  2015          31   \n",
      "150             0             0          5      7   31  2015          31   \n",
      "180             2             0          5      7   31  2015          31   \n",
      "210             0             0          5      7   31  2015          31   \n",
      "240             2             0          5      7   31  2015          31   \n",
      "270             0             0          5      7   31  2015          31   \n",
      "300             2             0          5      7   31  2015          31   \n",
      "330             2             0          5      7   31  2015          31   \n",
      "360             0             0          5      7   31  2015          31   \n",
      "390             0             0          5      7   31  2015          31   \n",
      "420             2             0          5      7   31  2015          31   \n",
      "450             2             0          5      7   31  2015          31   \n",
      "480             0             0          5      7   31  2015          31   \n",
      "510             2             0          5      7   31  2015          31   \n",
      "540             2             0          5      7   31  2015          31   \n",
      "570             0             0          5      7   31  2015          31   \n",
      "600             2             0          5      7   31  2015          31   \n",
      "630             0             0          5      7   31  2015          31   \n",
      "660             0             0          5      7   31  2015          31   \n",
      "690             2             0          5      7   31  2015          31   \n",
      "720             0             0          5      7   31  2015          31   \n",
      "750             0             0          5      7   31  2015          31   \n",
      "780             0             0          5      7   31  2015          31   \n",
      "810             0             0          5      7   31  2015          31   \n",
      "840             2             0          5      7   31  2015          31   \n",
      "870             0             0          5      7   31  2015          31   \n",
      "...           ...           ...        ...    ...  ...   ...         ...   \n",
      "32579           0             0          4      7    2  2015          27   \n",
      "32609           2             0          4      7    2  2015          27   \n",
      "32639           0             0          4      7    2  2015          27   \n",
      "32669           0             0          4      7    2  2015          27   \n",
      "32699           0             0          4      7    2  2015          27   \n",
      "32729           2             0          4      7    2  2015          27   \n",
      "32759           0             0          4      7    2  2015          27   \n",
      "32789           2             0          4      7    2  2015          27   \n",
      "32819           0             0          4      7    2  2015          27   \n",
      "32849           0             0          4      7    2  2015          27   \n",
      "32879           2             0          4      7    2  2015          27   \n",
      "32909           1             0          4      7    2  2015          27   \n",
      "32939           0             0          4      7    2  2015          27   \n",
      "32969           2             0          4      7    2  2015          27   \n",
      "32999           0             0          4      7    2  2015          27   \n",
      "33029           2             0          4      7    2  2015          27   \n",
      "33059           0             0          4      7    2  2015          27   \n",
      "33089           2             0          4      7    2  2015          27   \n",
      "33119           0             0          4      7    2  2015          27   \n",
      "33149           2             0          4      7    2  2015          27   \n",
      "33179           2             0          4      7    2  2015          27   \n",
      "33209           0             0          4      7    2  2015          27   \n",
      "33239           0             0          4      7    2  2015          27   \n",
      "33269           0             0          4      7    2  2015          27   \n",
      "33299           2             0          4      7    2  2015          27   \n",
      "33329           0             0          4      7    2  2015          27   \n",
      "33359           2             0          4      7    2  2015          27   \n",
      "33389           2             0          4      7    2  2015          27   \n",
      "33419           2             0          4      7    2  2015          27   \n",
      "33449           2             0          4      7    2  2015          27   \n",
      "\n",
      "       IsPromoMonth  StoreClosedNextDay  \n",
      "0                 0                   0  \n",
      "30                1                   0  \n",
      "60                1                   0  \n",
      "90                0                   0  \n",
      "120               0                   0  \n",
      "150               0                   0  \n",
      "180               0                   0  \n",
      "210               0                   0  \n",
      "240               0                   0  \n",
      "270               0                   0  \n",
      "300               1                   0  \n",
      "330               1                   0  \n",
      "360               0                   0  \n",
      "390               1                   0  \n",
      "420               1                   0  \n",
      "450               0                   0  \n",
      "480               1                   0  \n",
      "510               1                   0  \n",
      "540               0                   0  \n",
      "570               1                   0  \n",
      "600               1                   0  \n",
      "630               1                   0  \n",
      "660               0                   0  \n",
      "690               1                   0  \n",
      "720               0                   0  \n",
      "750               0                   0  \n",
      "780               1                   0  \n",
      "810               0                   0  \n",
      "840               0                   0  \n",
      "870               0                   0  \n",
      "...             ...                 ...  \n",
      "32579             0                   0  \n",
      "32609             0                   0  \n",
      "32639             1                   0  \n",
      "32669             0                   0  \n",
      "32699             1                   0  \n",
      "32729             0                   0  \n",
      "32759             1                   0  \n",
      "32789             0                   0  \n",
      "32819             1                   0  \n",
      "32849             1                   0  \n",
      "32879             0                   0  \n",
      "32909             0                   0  \n",
      "32939             0                   0  \n",
      "32969             1                   0  \n",
      "32999             1                   0  \n",
      "33029             0                   0  \n",
      "33059             1                   0  \n",
      "33089             0                   0  \n",
      "33119             1                   0  \n",
      "33149             0                   0  \n",
      "33179             1                   0  \n",
      "33209             1                   0  \n",
      "33239             0                   0  \n",
      "33269             1                   0  \n",
      "33299             0                   0  \n",
      "33329             1                   0  \n",
      "33359             0                   0  \n",
      "33389             0                   0  \n",
      "33419             0                   0  \n",
      "33449             0                   0  \n",
      "\n",
      "[33450 rows x 16 columns]\n",
      "8700       291\n",
      "26220      875\n",
      "8701      1406\n",
      "26221     1990\n",
      "8702      2521\n",
      "26222     3105\n",
      "8703      3636\n",
      "26223     4220\n",
      "8704      4751\n",
      "26224     5335\n",
      "33424     5575\n",
      "5         5576\n",
      "35        5577\n",
      "65        5578\n",
      "95        5579\n",
      "125       5580\n",
      "155       5581\n",
      "185       5582\n",
      "215       5583\n",
      "245       5584\n",
      "275       5585\n",
      "305       5586\n",
      "335       5587\n",
      "365       5588\n",
      "395       5589\n",
      "425       5590\n",
      "455       5591\n",
      "485       5592\n",
      "515       5593\n",
      "545       5594\n",
      "         ...  \n",
      "32576    30076\n",
      "32606    30077\n",
      "32636    30078\n",
      "32666    30079\n",
      "32696    30080\n",
      "32726    30081\n",
      "32756    30082\n",
      "32786    30083\n",
      "32816    30084\n",
      "32846    30085\n",
      "32906    30087\n",
      "32966    30089\n",
      "32996    30090\n",
      "33026    30091\n",
      "33056    30092\n",
      "33086    30093\n",
      "33116    30094\n",
      "33146    30095\n",
      "33176    30096\n",
      "33206    30097\n",
      "33236    30098\n",
      "33266    30099\n",
      "33296    30100\n",
      "33326    30101\n",
      "33356    30102\n",
      "33386    30103\n",
      "33416    30104\n",
      "27237    31013\n",
      "27238    32128\n",
      "27239    33243\n",
      "Name: Id, Length: 4377, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test[test['StoreClosedNextDay'] == 1]['Id'])\n",
    "# print(closed_index)\n",
    "\"\"\"\n",
    "Functions Wrappers that return models\n",
    "\"\"\"\n",
    "\n",
    "def rmspe(pred, labels):\n",
    "    return np.float32(np.sqrt(np.mean((pred/labels-1) ** 2)))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyper Parameter Optimisation\n",
    "\n",
    "Choosing hyper parameters for models\n",
    "\"\"\"\n",
    "params_rf = {'n_estimators':  [10, 100, 500, 1000],\n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'max_depth': [8,9,10,11,12]\n",
    "            }\n",
    "\n",
    "params_xgb = {'n_estimators': [100, 300, 500, 1000],\n",
    "              'learning_rate': [0.1, 0.2, 0.5],\n",
    "              'max_depth': [8, 9, 10, 12],\n",
    "              \"subsample\": [0.8],\n",
    "              \"colsample_bytree\": [0.7],\n",
    "              \"seed\": [3244],\n",
    "           }\n",
    "\n",
    "def optimise_hyper_parameters(train, labels, models, k, scoring_fn):\n",
    "    clfs = []\n",
    "    cv_sets = TimeSeriesSplit(n_splits=k).split(train)\n",
    "    for idx, model_params in enumerate(models):\n",
    "        params, model = model_params\n",
    "        clf = GridSearchCV(model, params, scoring=scoring_fn, cv=cv_sets)\n",
    "        fitted = clf.fit(train, labels.ravel())\n",
    "        clfs.append(fitted)\n",
    "    return clfs\n",
    "\n",
    "\n",
    "models = [(params_rf, RandomForestRegressor()), (params_xgb, xgb.XGBRegressor())]\n",
    "optimal_models = optimise_hyper_parameters(train, np.log1p(labels), models, 5, make_scorer(rmspe))\n",
    "\n",
    "for optimal_model in optimal_models:\n",
    "    print(optimal_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Selection, Ensembling and Local Validation Result\n",
    "\n",
    "We set parameters here again so that we do not have to rerun the above cell.\n",
    "Hyper parameter tuning is time costly.\n",
    "\"\"\"\n",
    "\n",
    "n = round(len(train)*0.012)\n",
    "X_train = train[n:] \n",
    "X_valid = train[:n]\n",
    "y_train = labels[n:]\n",
    "y_valid = labels[:n]\n",
    "y_train = np.log1p(np.array(y_train, dtype=np.int32))\n",
    "y_valid = np.log1p(np.array(y_valid, dtype=np.int32))\n",
    "\n",
    "## XGBoost Results using optimal parameters selected above\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"eta\": 0.2,\n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1,\n",
    "          \"seed\": 3244,\n",
    "          }\n",
    "\n",
    "num_boost_round = 300\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "xg_model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=False)\n",
    "\n",
    "## Local Validation\n",
    "pred_y = xg_model.predict(xgb.DMatrix(X_valid))\n",
    "error = rmspe(np.expm1(y_valid), np.expm1(pred_y))\n",
    "print('XGB RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "\n",
    "## Random Forest Results by using optimal parameters selected above\n",
    "rf_model = RandomForestRegressor(n_estimators=10, max_depth=8, max_features='log2')\n",
    "rf_model.fit(X_train,y_train)\n",
    "pred_y = rf_model.predict(X_valid)\n",
    "error = rmspe(np.expm1(y_valid), np.expm1(pred_y))\n",
    "print('RF RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "## SVR Results by using arbitrary parameters. Hyper Parameter Selection takes too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Submission Code\n",
    "\"\"\"\n",
    "# Preferably we can have a final model that is not just xgb. Or using the CV code above\n",
    "\n",
    "full_matrix = xgb.DMatrix(train, np.log1p(labels))\n",
    "final_model = xgb.train(params, full_matrix, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=False) \n",
    "dtest = xgb.DMatrix(test.drop('Id', axis=1))\n",
    "output = final_model.predict(dtest)\n",
    "\n",
    "result = pd.DataFrame({'Id': test.Id,'Sales': np.expm1(output)})\n",
    "\n",
    "## Set closed stores to have 0 sales\n",
    "for i in closed_index:\n",
    "    result.ix[result.Id == (i+1), 'Sales'] = 0\n",
    "\n",
    "## Make a Submission\n",
    "result.to_csv(\"The Learning Machine.csv\", index=False)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://datascience.stackexchange.com/questions/9777/one-hot-vector-representation-vs-label-encoding-for-categorical-variables\n",
    "- https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
