{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crphang/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "## importing the models that will be trialed and tested against validation dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.read_csv('store.csv')\n",
    "train = pd.read_csv('train.csv',dtype={\"StateHoliday\": str})\n",
    "test = pd.read_csv('test.csv',dtype={\"StateHoliday\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions that help initialise data and include new features to the data set to be trained\n",
    "\"\"\"\n",
    "def initialise_train_data(train, store):\n",
    "    ## label our dataframe with id\n",
    "    train['Id'] = range(1,len(train)+1)\n",
    "    ## fill the N.A.N values\n",
    "    store = store.fillna(0)\n",
    "    ## combine all features together\n",
    "    df = train.merge(store, on='Store')\n",
    "    ## Get labels and remove from dataframe\n",
    "    labels = df.values[:,3]\n",
    "    labels = np.array([labels], dtype=np.int32).T\n",
    "\n",
    "    return (df, labels)\n",
    "\n",
    "def initialise_test_data(test, store):\n",
    "    ## label our dataframe with id\n",
    "    test['Id'] = range(1,len(test)+1)\n",
    "    ## fill the N.A.N values\n",
    "    store = store.fillna(0)\n",
    "    ## combine all features together\n",
    "    df = test.merge(store, on='Store')\n",
    "    df = df.sort_values(['Id'], ascending=[1])\n",
    "    return df\n",
    "\n",
    "def set_store_close_and_sales_data(df):\n",
    "    df['StoreClosedNextDay'] = df['Open']\n",
    "    open_store_info = df['StoreClosedNextDay']\n",
    "    for i in range(len(open_store_info)):\n",
    "        if i+1 == len(open_store_info):\n",
    "            open_store_info[i] = 0\n",
    "            break\n",
    "        open_store_info[i] = -1 * (open_store_info[i+1] - 1)\n",
    "    if 'Sales' in df.columns:\n",
    "        df = df[df.Sales != 0]\n",
    "        df.drop('Sales', axis=1)\n",
    "\n",
    "## converting dates into year, month, day and additional feature week of the year\n",
    "def date_convert(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['WeekOfYear'] = df['Date'].dt.weekofyear\n",
    "    return df\n",
    "\n",
    "## adjust and standardise the mappings for all the categorical variables.\n",
    "def mapping_encoding(df):\n",
    "    mappings = {'0': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4}\n",
    "    \n",
    "    ## replace with values so they can be one hot encoded\n",
    "    df.StoreType.replace(mappings, inplace=True)\n",
    "    df.Assortment.replace(mappings, inplace=True)\n",
    "    df.StateHoliday.replace(mappings, inplace=True)\n",
    "    df.PromoInterval.replace(mappings, inplace=True)\n",
    "    \n",
    "    df['StateHoliday'] = LabelEncoder().fit_transform(df['StateHoliday'])\n",
    "    df['Assortment'] = LabelEncoder().fit_transform(df['Assortment'])\n",
    "    df['StoreType'] = LabelEncoder().fit_transform(df['StoreType'])\n",
    "    return df\n",
    "\n",
    "def store_features(df):\n",
    "    mappings_month = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n",
    "                      7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    df['monthStr'] = df.Month.map(mappings_month)\n",
    "    df.loc[df.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    df['IsPromoMonth'] = 0\n",
    "    for interval in df.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                df.loc[(df.monthStr == month) & (df.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "    return df\n",
    "\n",
    "def select_features(df, feature_type):\n",
    "    train_features = ['Store', 'Promo', 'Promo2', 'Customers', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'IsPromoMonth', 'StoreClosedNextDay']\n",
    "    test_features = ['Id', 'Store', 'Promo', 'Promo2', 'Customers', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'IsPromoMonth', 'StoreClosedNextDay']\n",
    "\n",
    "    if feature_type == 'train':\n",
    "        return df[train_features]\n",
    "    return df[test_features]\n",
    "\n",
    "## check for rows which stores are closed\n",
    "def get_closed_stores_index(df):\n",
    "    return df.ix[df['Open']==0].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crphang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/home/crphang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build Features in the order defined by feature_builders array\n",
    "\"\"\"\n",
    "closed_index = get_closed_stores_index(test)\n",
    "\n",
    "train, labels = initialise_train_data(train, store)\n",
    "test = initialise_test_data(test, store)\n",
    "\n",
    "feature_builders = [set_store_close_and_sales_data, date_convert, mapping_encoding, store_features]\n",
    "\n",
    "for i in range(len(feature_builders)):\n",
    "    train = feature_builders[i](train)\n",
    "    test = feature_builders[i](test)\n",
    "\n",
    "train = select_features(train, 'train')\n",
    "test = select_features(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions Wrappers that return models\n",
    "\"\"\"\n",
    "\n",
    "def rmspe(pred, labels):\n",
    "    return np.float32(np.sqrt(np.mean((pred/labels-1) ** 2)))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyper Parameter Optimisation\n",
    "\n",
    "Choosing hyper parameters for models\n",
    "\"\"\"\n",
    "params_rf = {'n_estimators':  [10, 100, 500, 1000],\n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'max_depth': [8,9,10,11,12]\n",
    "            }\n",
    "\n",
    "params_xgb = {'n_estimators': [100, 300, 500, 1000],\n",
    "              'learning_rate': [0.1, 0.2, 0.5],\n",
    "              'max_depth': [8, 9, 10, 12],\n",
    "              \"subsample\": [0.8],\n",
    "              \"colsample_bytree\": [0.7],\n",
    "              \"seed\": [3244],\n",
    "           }\n",
    "\n",
    "def optimise_hyper_parameters(train, labels, models, k, scoring_fn):\n",
    "    clfs = []\n",
    "    cv_sets = TimeSeriesSplit(n_splits=k).split(train)\n",
    "    for idx, model_params in enumerate(models):\n",
    "        params, model = model_params\n",
    "        clf = GridSearchCV(model, params, scoring=scoring_fn, cv=cv_sets)\n",
    "        fitted = clf.fit(train, labels.ravel())\n",
    "        clfs.append(fitted)\n",
    "    return clfs\n",
    "\n",
    "\n",
    "models = [(params_rf, RandomForestRegressor()), (params_xgb, xgb.XGBRegressor())]\n",
    "optimal_models = optimise_hyper_parameters(train, np.log1p(labels), models, 5, make_scorer(rmspe))\n",
    "\n",
    "for optimal_model in optimal_models:\n",
    "    print(optimal_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Selection, Ensembling and Local Validation Result\n",
    "\n",
    "We set parameters here again so that we do not have to rerun the above cell.\n",
    "Hyper parameter tuning is time costly.\n",
    "\"\"\"\n",
    "\n",
    "n = round(len(train)*0.012)\n",
    "X_train = train[n:] \n",
    "X_valid = train[:n]\n",
    "y_train = labels[n:]\n",
    "y_valid = labels[:n]\n",
    "y_train = np.log1p(np.array(y_train, dtype=np.int32))\n",
    "y_valid = np.log1p(np.array(y_valid, dtype=np.int32))\n",
    "\n",
    "## XGBoost Results using optimal parameters selected above\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"eta\": 0.2,\n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1,\n",
    "          \"seed\": 3244,\n",
    "          }\n",
    "\n",
    "num_boost_round = 300\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "xg_model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=False)\n",
    "\n",
    "## Local Validation\n",
    "pred_y = xg_model.predict(xgb.DMatrix(X_valid))\n",
    "error = rmspe(np.expm1(y_valid), np.expm1(pred_y))\n",
    "print('XGB RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "\n",
    "## Random Forest Results by using optimal parameters selected above\n",
    "rf_model = RandomForestRegressor(n_estimators=10, max_depth=8, max_features='log2')\n",
    "rf_model.fit(X_train,y_train)\n",
    "pred_y = rf_model.predict(X_valid)\n",
    "error = rmspe(np.expm1(y_valid), np.expm1(pred_y))\n",
    "print('RF RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "## SVR Results by using arbitrary parameters. Hyper Parameter Selection takes too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Submission Code\n",
    "\"\"\"\n",
    "# Preferably we can have a final model that is not just xgb. Or using the CV code above\n",
    "\n",
    "full_matrix = xgb.DMatrix(train, np.log1p(labels))\n",
    "final_model = xgb.train(params, full_matrix, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=False) \n",
    "dtest = xgb.DMatrix(test.drop('Id', axis=1))\n",
    "output = final_model.predict(dtest)\n",
    "\n",
    "result = pd.DataFrame({'Id': test.Id,'Sales': np.expm1(output)})\n",
    "\n",
    "## Set closed stores to have 0 sales\n",
    "for i in closed_index:\n",
    "    result.ix[result.Id == (i+1), 'Sales'] = 0\n",
    "\n",
    "## Make a Submission\n",
    "result.to_csv(\"The Learning Machine.csv\", index=False)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://datascience.stackexchange.com/questions/9777/one-hot-vector-representation-vs-label-encoding-for-categorical-variables\n",
    "- https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
